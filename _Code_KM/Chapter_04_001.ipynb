{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d8a304-c79d-498c-9031-7a18551b52b1",
   "metadata": {},
   "source": [
    "# Chapter 4 - In Text Examples and Musings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a57a67-94ca-46d1-9611-82f373885638",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a7ce8f7-0a74-4a0b-8257-e06b15e19d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a763bf-d147-4bc0-b991-faa35f22f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr = imageio.imread('/home/kamil/_LEARNING/dlwpt-code/data/p1ch4/image-dog/bobby.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea70a73-3e31-46d2-ba79-d24c6360ade9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb41576-5870-474b-b0c2-37c6d8388a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can change the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4fc157f-e851-4c7a-a7c4-d8513b4f689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48a15a7d-aff8-4ab5-a2ce-f24f4d833bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.from_numpy(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b48c85b-dba7-4079-8aa5-a2ccf9094873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([720, 1280, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22b9661b-ff53-4001-91cc-722c1811ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = img.permute(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2885ff3-476e-4072-964a-caa988bd22cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 720, 1280])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c28c1a59-1989-4e51-aebb-49e4bc48f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c083bbb4-ebdb-467e-b83f-4117609e1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/home/kamil/_LEARNING/dlwpt-code/data/p1ch4/image-cats/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ce2e16f-0366-44a7-b19b-4576fec12d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75962890-a503-464e-b8b1-556fb5cbe1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "911f0178-0342-4a15-bfaa-1647b9d95bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,filename in enumerate(data_dir.glob('*.*')):\n",
    "    img_arr = imageio.imread(filename)\n",
    "    img_t = torch.from_numpy(img_arr)\n",
    "    img_t = img_t.permute(2,0,1) # change order to Channel, Height, Width from Height, Width, Channel\n",
    "    img_t = img_t[:3] # take only the first three channels, avoiding others like alpha\n",
    "    batch[i] = img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e467008-a770-46ce-85f1-671bbffd9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaaf163e-0fba-4d73-8e7a-9c723d868ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a390bdaa-cc26-47cc-83f8-332a74fa607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per channel z-score\n",
    "n_channels = batch.shape[1] # get the number of channels\n",
    "for c in range(n_channels): # loop over each channel\n",
    "    mean = torch.mean(batch[:,c]) # get the mean of the channel for all batches: all batches, all rows, all widths for a specific channel\n",
    "    std = torch.std(batch[:,c]) # same as above, but standard deviation\n",
    "    batch[:,c] = (batch[:,c] - mean) / std # update the batch, now every pixel in that channel will have 0-mean and 1-stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5a8520b-bfb2-4bff-baf0-67e30ac45d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.1 volumetric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3445639-4fed-45aa-9e9f-2d0e8f5162f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = Path('/home/kamil/_LEARNING/dlwpt-code/data/p1ch4/volumetric-dicom/2-LUNG 3.0  B70f-04083/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3bcf3ef-0ce5-4fa4-adec-b99317471803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DICOM (examining files): 1/99 files (1.0%99/99 files (100.0%)\n",
      "  Found 1 correct series.\n",
      "Reading DICOM (loading data): 99/99  (100.0%)\n"
     ]
    }
   ],
   "source": [
    "vol_arr = imageio.volread(dir_path, 'DICOM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0637b00-eb20-48f4-82e1-bd70ecfb0605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 512, 512)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfe3e4da-2b0f-4f2f-94eb-240994197538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 99 images, each 512x512, no color channel this case because it's greyscale and omitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ea569dd-0b87-48e3-bb1c-41ff5e237db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch is expecting a channel dimension so we will need to add that in via unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e91b409e-cf7f-45b3-b805-cae0c4d90a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([99, 512, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol = torch.from_numpy(vol_arr)\n",
    "vol = vol.float()\n",
    "vol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e79de654-4a4f-4da0-a2e7-a04e3c915fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 99, 512, 512])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol = torch.unsqueeze(vol, 0) # add a channel dimension, so we should have Channel=1, Depth=99, Height=512, Width=512\n",
    "vol.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6178eb-f638-4a20-9035-6ac355734ac1",
   "metadata": {},
   "source": [
    "#### 4.3 Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7394968-71cf-4ee1-9ec6-bce2b56b6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = Path('/home/kamil/_LEARNING/dlwpt-code/data/p1ch4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4db7faae-8511-4773-b0f2-8701dd3fdf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da0f2f5d-9c85-4d00-8e9a-899a16223c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_path = BASE / 'tabular-wine' / 'winequality-white.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7074f79e-822e-4ec3-bac0-9764fc401b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "589f9b90-a2ea-408a-b801-f729135ea285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0b5db42-87b9-4524-97f4-7c0d4cb46f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=';', skiprows=1) # cast to float32 so tensor is correct type, skip the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6b9fe3e-a653-4243-b557-909b9997c949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],\n",
       "       [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],\n",
       "       [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],\n",
       "       ...,\n",
       "       [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],\n",
       "       [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],\n",
       "       [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineq_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e62f4048-dbb8-455c-ac6a-7212b55763f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "608dd3eb-5e8e-4612-aa66-b07afead7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = next(csv.reader(open(wine_path), delimiter=';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0187339-3593-43ba-94bd-e1133f99c24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fixed acidity',\n",
       " 'volatile acidity',\n",
       " 'citric acid',\n",
       " 'residual sugar',\n",
       " 'chlorides',\n",
       " 'free sulfur dioxide',\n",
       " 'total sulfur dioxide',\n",
       " 'density',\n",
       " 'pH',\n",
       " 'sulphates',\n",
       " 'alcohol',\n",
       " 'quality']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "111c34a1-5197-453c-9ed5-794250114b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4898, 12)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineq_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3065bc6-db99-4529-a4df-e5a13a977ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load to tensors\n",
    "wineq = torch.from_numpy(wineq_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f35f5ae6-0bc6-4281-9a59-857ba7a6f3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 12]), torch.float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineq.shape, wineq.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "169e59f9-4d47-4916-ad7f-50b718b4bfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4898, 11])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get score and features\n",
    "data = wineq[:,:-1] # select all rows, but not the last column\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "635dca8d-55f4-48b2-befb-c9fc6a8ac854",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = wineq[:,-1] # select all rows, ONLY the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d874ee57-f028-4e95-9258-95fa6e344132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4898])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39504b70-1dd4-4a2b-b5a8-5e8b51d5ee41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 6., 6.,  ..., 6., 7., 6.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b34bf12-c045-40c0-b587-e17e85fd44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 2 options: regress to get a score or classify to get a score\n",
    "# let's one-hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f79af8b-1ba0-4c16-be98-b89f26f668ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to int\n",
    "target = target.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa92748e-b2d8-47f9-acce-fdc43ebdbec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 6,  ..., 6, 7, 6])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2631960f-827a-4942-aee1-597f2026d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_onehot = torch.zeros(target.shape[0], 10) # 10 b/c we have that many distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cfc5fb7d-03ba-45df-9baa-5850165686a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6aa8f7e8-e93b-4fba-8fde-7d8debb86fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot.scatter(1, target.unsqueeze(1), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bebad501-3703-4215-992b-1a3f57d424f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 6,  ..., 6, 7, 6])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f12d7da5-de14-425c-bc74-ed51fb8db9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6],\n",
       "        [6],\n",
       "        [6],\n",
       "        ...,\n",
       "        [6],\n",
       "        [7],\n",
       "        [6]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "71737f39-6286-4acc-8204-37e59db070bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_onehot = target_onehot.scatter(1, target.unsqueeze(1), 1.0)\n",
    "# or use scatter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "202b553c-7da7-4d15-a076-dd2866f17059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5519540-5e4a-49e0-aa5b-88a40b0d0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "03eef022-346e-470b-aad6-0a086a22d303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4898])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c1cbcc00-762f-432f-aee4-c06e4dd954f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.8548e+00, 2.7824e-01, 3.3419e-01, 6.3914e+00, 4.5772e-02, 3.5308e+01,\n",
       "        1.3836e+02, 9.9403e-01, 3.1883e+00, 4.8985e-01, 1.0514e+01])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mean = data.mean(dim=0) # dim=0 makes us calculate for each column, without it we would get a single value\n",
    "data_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0e6af658-5cec-409c-8f5f-e7b5e7219235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.4387e-01, 1.0079e-01, 1.2102e-01, 5.0721e+00, 2.1848e-02, 1.7007e+01,\n",
       "        4.2498e+01, 2.9909e-03, 1.5100e-01, 1.1413e-01, 1.2306e+00])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_std = data.std(dim=0)\n",
    "data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7306087e-5bbb-42d2-b10e-043ae99f2201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7209e-01, -8.1764e-02,  2.1325e-01,  ..., -1.2468e+00,\n",
       "         -3.4914e-01, -1.3930e+00],\n",
       "        [-6.5743e-01,  2.1587e-01,  4.7991e-02,  ...,  7.3992e-01,\n",
       "          1.3467e-03, -8.2418e-01],\n",
       "        [ 1.4756e+00,  1.7448e-02,  5.4378e-01,  ...,  4.7502e-01,\n",
       "         -4.3677e-01, -3.3662e-01],\n",
       "        ...,\n",
       "        [-4.2042e-01, -3.7940e-01, -1.1915e+00,  ..., -1.3131e+00,\n",
       "         -2.6152e-01, -9.0544e-01],\n",
       "        [-1.6054e+00,  1.1666e-01, -2.8253e-01,  ...,  1.0048e+00,\n",
       "         -9.6250e-01,  1.8574e+00],\n",
       "        [-1.0129e+00, -6.7703e-01,  3.7852e-01,  ...,  4.7502e-01,\n",
       "         -1.4882e+00,  1.0448e+00]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_normd = (data - data_mean)/data_std\n",
    "data_normd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283d51f-4ab0-4566-a50d-aff8f0e12411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding thresholds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df5328a-3aaf-475e-8836-3827acf4c39b",
   "metadata": {},
   "source": [
    "#### 4.4 Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5cfbae86-3597-4ed6-ad22-da127d81c53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000e+00, 1.0000e+00, 1.0000e+00, ..., 3.0000e+00, 1.3000e+01,\n",
       "        1.6000e+01],\n",
       "       [2.0000e+00, 1.0000e+00, 1.0000e+00, ..., 8.0000e+00, 3.2000e+01,\n",
       "        4.0000e+01],\n",
       "       [3.0000e+00, 1.0000e+00, 1.0000e+00, ..., 5.0000e+00, 2.7000e+01,\n",
       "        3.2000e+01],\n",
       "       ...,\n",
       "       [1.7377e+04, 3.1000e+01, 1.0000e+00, ..., 7.0000e+00, 8.3000e+01,\n",
       "        9.0000e+01],\n",
       "       [1.7378e+04, 3.1000e+01, 1.0000e+00, ..., 1.3000e+01, 4.8000e+01,\n",
       "        6.1000e+01],\n",
       "       [1.7379e+04, 3.1000e+01, 1.0000e+00, ..., 1.2000e+01, 3.7000e+01,\n",
       "        4.9000e+01]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes_np = np.loadtxt(BASE/'bike-sharing-dataset'/'hour-fixed.csv', \n",
    "                      dtype=np.float32, \\\n",
    "                      delimiter=',', \\\n",
    "                      skiprows=1, \\\n",
    "                      converters={1: lambda x: float(x[8:10])})\n",
    "bikes_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a4f5dd05-a633-4eba-a046-16b2b25160ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.0000e+00, 1.3000e+01,\n",
       "         1.6000e+01],\n",
       "        [2.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 8.0000e+00, 3.2000e+01,\n",
       "         4.0000e+01],\n",
       "        [3.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 5.0000e+00, 2.7000e+01,\n",
       "         3.2000e+01],\n",
       "        ...,\n",
       "        [1.7377e+04, 3.1000e+01, 1.0000e+00,  ..., 7.0000e+00, 8.3000e+01,\n",
       "         9.0000e+01],\n",
       "        [1.7378e+04, 3.1000e+01, 1.0000e+00,  ..., 1.3000e+01, 4.8000e+01,\n",
       "         6.1000e+01],\n",
       "        [1.7379e+04, 3.1000e+01, 1.0000e+00,  ..., 1.2000e+01, 3.7000e+01,\n",
       "         4.9000e+01]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes  = torch.from_numpy(bikes_np)\n",
    "bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "385d8800-94a8-4de3-91ed-d14c45eaa8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([17520, 17]), (17, 1))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes.shape, bikes.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9e4d65f6-3af9-4f0e-9fd8-499944c3783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# that is 17,520 hourly obersvations of 17 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d1009893-c2f9-419b-9868-789c60e1068f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([730, 24, 17]), (408, 17, 1))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's reshape it so that we have 3-dimensions, batch, 24 hours, 17 features\n",
    "daily_bikes = bikes.view(-1, 24, bikes.shape[1]) # -1 --> make it fit, 24 hours , bikes.shape[1] --> 17\n",
    "daily_bikes.shape, daily_bikes.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d1c37c3e-75fd-476e-8dc7-ffbe24b4ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pausing for now. next step is to one-hot encode the weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b2f02-96c0-44ad-8c3a-e472616a175c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k_dlwpt",
   "language": "python",
   "name": "conda-env-k_dlwpt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

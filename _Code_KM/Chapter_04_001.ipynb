{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d8a304-c79d-498c-9031-7a18551b52b1",
   "metadata": {},
   "source": [
    "# Chapter 4 - In Text Examples and Musings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a57a67-94ca-46d1-9611-82f373885638",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a7ce8f7-0a74-4a0b-8257-e06b15e19d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a763bf-d147-4bc0-b991-faa35f22f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr = imageio.imread('/home/kamil/_LEARNING/dlwpt-code/data/p1ch4/image-dog/bobby.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea70a73-3e31-46d2-ba79-d24c6360ade9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb41576-5870-474b-b0c2-37c6d8388a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can change the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4fc157f-e851-4c7a-a7c4-d8513b4f689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48a15a7d-aff8-4ab5-a2ce-f24f4d833bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.from_numpy(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b48c85b-dba7-4079-8aa5-a2ccf9094873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([720, 1280, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22b9661b-ff53-4001-91cc-722c1811ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = img.permute(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2885ff3-476e-4072-964a-caa988bd22cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 720, 1280])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c28c1a59-1989-4e51-aebb-49e4bc48f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c083bbb4-ebdb-467e-b83f-4117609e1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/home/kamil/_LEARNING/dlwpt-code/data/p1ch4/image-cats/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ce2e16f-0366-44a7-b19b-4576fec12d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75962890-a503-464e-b8b1-556fb5cbe1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "911f0178-0342-4a15-bfaa-1647b9d95bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,filename in enumerate(data_dir.glob('*.*')):\n",
    "    img_arr = imageio.imread(filename)\n",
    "    img_t = torch.from_numpy(img_arr)\n",
    "    img_t = img_t.permute(2,0,1) # change order to Channel, Height, Width from Height, Width, Channel\n",
    "    img_t = img_t[:3] # take only the first three channels, avoiding others like alpha\n",
    "    batch[i] = img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e467008-a770-46ce-85f1-671bbffd9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaaf163e-0fba-4d73-8e7a-9c723d868ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a390bdaa-cc26-47cc-83f8-332a74fa607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per channel z-score\n",
    "n_channels = batch.shape[1] # get the number of channels\n",
    "for c in range(n_channels): # loop over each channel\n",
    "    mean = torch.mean(batch[:,c]) # get the mean of the channel for all batches: all batches, all rows, all widths for a specific channel\n",
    "    std = torch.std(batch[:,c]) # same as above, but standard deviation\n",
    "    batch[:,c] = (batch[:,c] - mean) / std # update the batch, now every pixel in that channel will have 0-mean and 1-stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5a8520b-bfb2-4bff-baf0-67e30ac45d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.1 volumetric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3445639-4fed-45aa-9e9f-2d0e8f5162f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = Path('/home/kamil/_LEARNING/dlwpt-code/data/p1ch4/volumetric-dicom/2-LUNG 3.0  B70f-04083/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3bcf3ef-0ce5-4fa4-adec-b99317471803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DICOM (examining files): 1/99 files (1.0%99/99 files (100.0%)\n",
      "  Found 1 correct series.\n",
      "Reading DICOM (loading data): 99/99  (100.0%)\n"
     ]
    }
   ],
   "source": [
    "vol_arr = imageio.volread(dir_path, 'DICOM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0637b00-eb20-48f4-82e1-bd70ecfb0605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 512, 512)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfe3e4da-2b0f-4f2f-94eb-240994197538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 99 images, each 512x512, no color channel this case because it's greyscale and omitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ea569dd-0b87-48e3-bb1c-41ff5e237db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch is expecting a channel dimension so we will need to add that in via unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e91b409e-cf7f-45b3-b805-cae0c4d90a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([99, 512, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol = torch.from_numpy(vol_arr)\n",
    "vol = vol.float()\n",
    "vol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e79de654-4a4f-4da0-a2e7-a04e3c915fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 99, 512, 512])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol = torch.unsqueeze(vol, 0) # add a channel dimension, so we should have Channel=1, Depth=99, Height=512, Width=512\n",
    "vol.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6178eb-f638-4a20-9035-6ac355734ac1",
   "metadata": {},
   "source": [
    "#### 4.3 Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7394968-71cf-4ee1-9ec6-bce2b56b6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = Path('/home/kamil/_LEARNING/dlwpt-code/data/p1ch4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4db7faae-8511-4773-b0f2-8701dd3fdf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da0f2f5d-9c85-4d00-8e9a-899a16223c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_path = BASE / 'tabular-wine' / 'winequality-white.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7074f79e-822e-4ec3-bac0-9764fc401b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "589f9b90-a2ea-408a-b801-f729135ea285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0b5db42-87b9-4524-97f4-7c0d4cb46f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=';', skiprows=1) # cast to float32 so tensor is correct type, skip the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6b9fe3e-a653-4243-b557-909b9997c949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],\n",
       "       [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],\n",
       "       [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],\n",
       "       ...,\n",
       "       [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],\n",
       "       [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],\n",
       "       [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineq_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e62f4048-dbb8-455c-ac6a-7212b55763f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "608dd3eb-5e8e-4612-aa66-b07afead7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = next(csv.reader(open(wine_path), delimiter=';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0187339-3593-43ba-94bd-e1133f99c24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fixed acidity',\n",
       " 'volatile acidity',\n",
       " 'citric acid',\n",
       " 'residual sugar',\n",
       " 'chlorides',\n",
       " 'free sulfur dioxide',\n",
       " 'total sulfur dioxide',\n",
       " 'density',\n",
       " 'pH',\n",
       " 'sulphates',\n",
       " 'alcohol',\n",
       " 'quality']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "111c34a1-5197-453c-9ed5-794250114b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4898, 12)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineq_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3065bc6-db99-4529-a4df-e5a13a977ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load to tensors\n",
    "wineq = torch.from_numpy(wineq_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f35f5ae6-0bc6-4281-9a59-857ba7a6f3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 12]), torch.float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineq.shape, wineq.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "169e59f9-4d47-4916-ad7f-50b718b4bfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4898, 11])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get score and features\n",
    "data = wineq[:,:-1] # select all rows, but not the last column\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "635dca8d-55f4-48b2-befb-c9fc6a8ac854",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = wineq[:,-1] # select all rows, ONLY the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d874ee57-f028-4e95-9258-95fa6e344132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4898])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39504b70-1dd4-4a2b-b5a8-5e8b51d5ee41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 6., 6.,  ..., 6., 7., 6.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b34bf12-c045-40c0-b587-e17e85fd44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 2 options: regress to get a score or classify to get a score\n",
    "# let's one-hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f79af8b-1ba0-4c16-be98-b89f26f668ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to int\n",
    "target = target.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa92748e-b2d8-47f9-acce-fdc43ebdbec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 6,  ..., 6, 7, 6])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2631960f-827a-4942-aee1-597f2026d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_onehot = torch.zeros(target.shape[0], 10) # 10 b/c we have that many distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cfc5fb7d-03ba-45df-9baa-5850165686a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6aa8f7e8-e93b-4fba-8fde-7d8debb86fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot.scatter(1, target.unsqueeze(1), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bebad501-3703-4215-992b-1a3f57d424f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 6,  ..., 6, 7, 6])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f12d7da5-de14-425c-bc74-ed51fb8db9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6],\n",
       "        [6],\n",
       "        [6],\n",
       "        ...,\n",
       "        [6],\n",
       "        [7],\n",
       "        [6]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "71737f39-6286-4acc-8204-37e59db070bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_onehot = target_onehot.scatter(1, target.unsqueeze(1), 1.0)\n",
    "# or use scatter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "202b553c-7da7-4d15-a076-dd2866f17059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5519540-5e4a-49e0-aa5b-88a40b0d0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "03eef022-346e-470b-aad6-0a086a22d303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4898])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c1cbcc00-762f-432f-aee4-c06e4dd954f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.8548e+00, 2.7824e-01, 3.3419e-01, 6.3914e+00, 4.5772e-02, 3.5308e+01,\n",
       "        1.3836e+02, 9.9403e-01, 3.1883e+00, 4.8985e-01, 1.0514e+01])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mean = data.mean(dim=0) # dim=0 makes us calculate for each column, without it we would get a single value\n",
    "data_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0e6af658-5cec-409c-8f5f-e7b5e7219235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.4387e-01, 1.0079e-01, 1.2102e-01, 5.0721e+00, 2.1848e-02, 1.7007e+01,\n",
       "        4.2498e+01, 2.9909e-03, 1.5100e-01, 1.1413e-01, 1.2306e+00])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_std = data.std(dim=0)\n",
    "data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7306087e-5bbb-42d2-b10e-043ae99f2201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7209e-01, -8.1764e-02,  2.1325e-01,  ..., -1.2468e+00,\n",
       "         -3.4914e-01, -1.3930e+00],\n",
       "        [-6.5743e-01,  2.1587e-01,  4.7991e-02,  ...,  7.3992e-01,\n",
       "          1.3467e-03, -8.2418e-01],\n",
       "        [ 1.4756e+00,  1.7448e-02,  5.4378e-01,  ...,  4.7502e-01,\n",
       "         -4.3677e-01, -3.3662e-01],\n",
       "        ...,\n",
       "        [-4.2042e-01, -3.7940e-01, -1.1915e+00,  ..., -1.3131e+00,\n",
       "         -2.6152e-01, -9.0544e-01],\n",
       "        [-1.6054e+00,  1.1666e-01, -2.8253e-01,  ...,  1.0048e+00,\n",
       "         -9.6250e-01,  1.8574e+00],\n",
       "        [-1.0129e+00, -6.7703e-01,  3.7852e-01,  ...,  4.7502e-01,\n",
       "         -1.4882e+00,  1.0448e+00]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_normd = (data - data_mean)/data_std\n",
    "data_normd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283d51f-4ab0-4566-a50d-aff8f0e12411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding thresholds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df5328a-3aaf-475e-8836-3827acf4c39b",
   "metadata": {},
   "source": [
    "#### 4.4 Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5cfbae86-3597-4ed6-ad22-da127d81c53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000e+00, 1.0000e+00, 1.0000e+00, ..., 3.0000e+00, 1.3000e+01,\n",
       "        1.6000e+01],\n",
       "       [2.0000e+00, 1.0000e+00, 1.0000e+00, ..., 8.0000e+00, 3.2000e+01,\n",
       "        4.0000e+01],\n",
       "       [3.0000e+00, 1.0000e+00, 1.0000e+00, ..., 5.0000e+00, 2.7000e+01,\n",
       "        3.2000e+01],\n",
       "       ...,\n",
       "       [1.7377e+04, 3.1000e+01, 1.0000e+00, ..., 7.0000e+00, 8.3000e+01,\n",
       "        9.0000e+01],\n",
       "       [1.7378e+04, 3.1000e+01, 1.0000e+00, ..., 1.3000e+01, 4.8000e+01,\n",
       "        6.1000e+01],\n",
       "       [1.7379e+04, 3.1000e+01, 1.0000e+00, ..., 1.2000e+01, 3.7000e+01,\n",
       "        4.9000e+01]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes_np = np.loadtxt(BASE/'bike-sharing-dataset'/'hour-fixed.csv', \n",
    "                      dtype=np.float32, \\\n",
    "                      delimiter=',', \\\n",
    "                      skiprows=1, \\\n",
    "                      converters={1: lambda x: float(x[8:10])})\n",
    "bikes_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a4f5dd05-a633-4eba-a046-16b2b25160ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.0000e+00, 1.3000e+01,\n",
       "         1.6000e+01],\n",
       "        [2.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 8.0000e+00, 3.2000e+01,\n",
       "         4.0000e+01],\n",
       "        [3.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 5.0000e+00, 2.7000e+01,\n",
       "         3.2000e+01],\n",
       "        ...,\n",
       "        [1.7377e+04, 3.1000e+01, 1.0000e+00,  ..., 7.0000e+00, 8.3000e+01,\n",
       "         9.0000e+01],\n",
       "        [1.7378e+04, 3.1000e+01, 1.0000e+00,  ..., 1.3000e+01, 4.8000e+01,\n",
       "         6.1000e+01],\n",
       "        [1.7379e+04, 3.1000e+01, 1.0000e+00,  ..., 1.2000e+01, 3.7000e+01,\n",
       "         4.9000e+01]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes  = torch.from_numpy(bikes_np)\n",
    "bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "385d8800-94a8-4de3-91ed-d14c45eaa8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([17520, 17]), (17, 1))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes.shape, bikes.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9e4d65f6-3af9-4f0e-9fd8-499944c3783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# that is 17,520 hourly obersvations of 17 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d1009893-c2f9-419b-9868-789c60e1068f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([730, 24, 17]), (408, 17, 1))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's reshape it so that we have 3-dimensions, batch, 24 hours, 17 features\n",
    "daily_bikes = bikes.view(-1, 24, bikes.shape[1]) # -1 --> make it fit, 24 hours , bikes.shape[1] --> 17\n",
    "daily_bikes.shape, daily_bikes.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d1c37c3e-75fd-476e-8dc7-ffbe24b4ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pausing for now. next step is to one-hot encode the weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "812b2f02-96c0-44ad-8c3a-e472616a175c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 17])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_day = bikes[:24].long() # slice off the first day and convert to ints\n",
    "first_day.shape # should be 1,24,17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "605f4cff-c8f3-47dd-946a-b5380a170451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 4])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_onehot = torch.zeros(first_day.shape[0],4) # should be 24, 4 --> and the 4 are for our 4 weather classifications of 1,2,3,4 \n",
    "weather_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9ffbea0a-0d74-4557-ad50-f375e12f99dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_day[:,9] # view a slice of all hours but only the index=9 (i.e. 10th) column, the weather column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "42a6fc8a-d720-4dd6-ab34-68a833767d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_onehot = weather_onehot.scatter_(\n",
    "    dim=1,\n",
    "    index=first_day[:,9].unsqueeze(1).long() - 1, \n",
    "    value=1.0) \n",
    "\n",
    "# along the axis=1\n",
    "# first day index columns, we  use unsqueeze to add extra dimension [[1],[1],...[2],[2]] and we subtract 1 to fix for indexing\n",
    "# populate with 1.0's\n",
    "weather_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "96742c0b-6f03-4215-9984-3925d5b58fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  6.0000,\n",
       "          0.0000,  1.0000,  0.2400,  0.2879,  0.8100,  0.0000,  3.0000, 13.0000,\n",
       "         16.0000,  1.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add this result back to our data\n",
    "torch.cat((bikes[:24],weather_onehot),1)[:1] # concatenate the first day with the one-hot encoded data, adding it along the column axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "38165ad3-5490-4b43-886e-f8f75660e8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last 4 entries are [1,0,0,0] which correspond to our one-hot encoded 1 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6d3e2c8c-c167-4afd-a8a6-5795d1b08c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to do this for the entire dataset we could follow the same procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2fd87e64-2055-498b-abdc-785302067e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([730, 4, 24])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build our daily weather slice\n",
    "daily_weather_one_hot = torch.zeros(daily_bikes.shape[0], 4, daily_bikes.shape[1]) # all the days, 4 columns, 24-hours\n",
    "daily_weather_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "73f0e603-44c5-4204-b4d7-d72862897649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([730, 24, 17])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_bikes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c4474dc3-ec4c-4fb3-bbe7-16d122b5c22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([730, 4, 24])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_one_hot = daily_weather_one_hot.scatter(\n",
    "    dim=1,\n",
    "    index=daily_bikes[:,:,9].unsqueeze(1).long()-1, # all days, all hours, (10th column) : give extra dimension, convert to int, subtract 1 for indexing\n",
    "    value=1.0)\n",
    "daily_weather_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2ef340ec-d298-4e2a-aa73-b4dde528a90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([730, 24, 17]), torch.Size([730, 4, 24]))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_bikes.shape, daily_weather_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2a5a287b-6252-4542-b74d-0e52b6c0ea02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([730, 24, 4])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to rearrange the daily weather one\n",
    "daily_weather_one_hot_reshape = torch.reshape(daily_weather_one_hot, (daily_weather_one_hot.shape[0],daily_weather_one_hot.shape[2],daily_weather_one_hot.shape[1]))\n",
    "daily_weather_one_hot_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1a092622-95c2-4f98-9351-a0bdd5fa0137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([730, 24, 21])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the one hot encoded weather as a column\n",
    "daily_bikes_one_hot = torch.cat((daily_bikes, daily_weather_one_hot_reshape),dim=2) # add one hot to daily bikes as a column\n",
    "\n",
    "daily_bikes_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6acacce-fc6f-4226-af4e-b25c84bb03cb",
   "metadata": {},
   "source": [
    "## 4.5 Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "41e4c449-b862-426c-b841-e36346d43a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASE/'jane-austen'/'1342-0.txt', encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "05b3bd0e-a2ce-4b9e-b508-ae20bfe47ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Impossible, Mr. Bennet, impossible, when I am not acquainted with him'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = text.split('\\n')\n",
    "line = lines[200]\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "40246cbf-8260-42d9-b0f0-8fb3047a417d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 128])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use ascii, so 128 character limit\n",
    "letter_t = torch.zeros(len(line),128)\n",
    "letter_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8c15d678-f0a8-418a-937f-b974d0ee4c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is now a matrix that holds will hold a one-hot encoded letter \n",
    "# we have 70 characters in the string which will be our rows, then we encode \n",
    "# those characters from our limited 128 character set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "571d17f1-249b-456f-b888-31e33bfdb9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, letter in enumerate(line.lower().strip()):\n",
    "    letter_index = ord(letter) if ord(letter) < 128 else 0 # ord() returns the unicode integer value, we are limiting ourselves to the first 128 unicode values which is the ASCII set\n",
    "    letter_t[i][letter_index] = 1 # for the ith letter at the ascii integer value write a 1 (otherwise all 0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "92d2ded5-bdee-43fd-8f5d-cb570776a787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m Return the Unicode code point for a one-character string.\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ord?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "66f6105c-d8fd-418f-8b63-1360d0076dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(input_str):\n",
    "    punctuation = '.,;:\"!?“”_-'\n",
    "    word_list = input_str.lower().replace('\\n',' ').split()\n",
    "    word_list = [word.strip(punctuation) for word in word_list]\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5192d3a1-bca4-40de-9221-c06e8e0471ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['impossible',\n",
       " 'mr',\n",
       " 'bennet',\n",
       " 'impossible',\n",
       " 'when',\n",
       " 'i',\n",
       " 'am',\n",
       " 'not',\n",
       " 'acquainted',\n",
       " 'with',\n",
       " 'him']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_line = clean_words(line)\n",
    "words_in_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6345e732-8132-4979-a767-dc832fd9d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = sorted(set(clean_words(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a08eb924-0e5a-482e-969e-4bc995b2edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index_dict = {word: i for (i,word) in enumerate(word_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f70e8bed-59d7-4f08-8145-f44ca2dc0394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7261, 3394)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index_dict), word2index_dict['impossible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e89c486f-9998-4203-b16b-5b6f0a5798e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 3394 impossible\n",
      " 1 4305 mr\n",
      " 2  813 bennet\n",
      " 3 3394 impossible\n",
      " 4 7078 when\n",
      " 5 3315 i\n",
      " 6  415 am\n",
      " 7 4436 not\n",
      " 8  239 acquainted\n",
      " 9 7148 with\n",
      "10 3215 him\n"
     ]
    }
   ],
   "source": [
    "# now we need to one-hot encode using our dictionary and corresponding index values\n",
    "word_t = torch.zeros(len(words_in_line), len(word2index_dict)) # row for each word in the line, column for each word in the vocabulary\n",
    "for i, word in enumerate(words_in_line): # index of the word in the line will correspond to the row\n",
    "    word_index = word2index_dict[word]\n",
    "    word_t[i][word_index] = 1\n",
    "    print('{:2} {:4} {}'.format(i,word_index,word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953d715f-5512-4551-9106-ca3d92b1ea24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k_dlwpt",
   "language": "python",
   "name": "conda-env-k_dlwpt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

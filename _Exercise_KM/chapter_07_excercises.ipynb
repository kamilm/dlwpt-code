{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb585c7-60aa-4023-b435-7041dfc81147",
   "metadata": {},
   "source": [
    "# Chapter 7 - Exercises "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc2650-d6e7-496e-802f-f342766a9e37",
   "metadata": {},
   "source": [
    "1. Use `torchvision` to implement random cropping of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f08944-da2d-4957-87a3-c26185ad42f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc1c80230d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision as tv\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb4b5ce-3abe-451f-a622-e7b8f769e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of this is copied from the notes workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "665638bf-5775-4c8c-be01-b517f9e30060",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data_unversioned/chapter07/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add52d1b-fbce-4b6d-8c56-f1165b931bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.array([0.49147, 0.48226798, 0.44678035], dtype=np.float32)\n",
    "stds = np.array([0.24703224, 0.24348514, 0.26158786], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5024d07e-f3bc-464b-80e0-530d52eeb1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformed_cifar10 = datasets.CIFAR10(data_path, train=True, download=True,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        transforms.RandomCrop(24, pad_if_needed=True, padding=8, fill=(0,0,0)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=means, std=stds),]))\n",
    "\n",
    "transformed_cifar10_val = datasets.CIFAR10(data_path, train=False, download=True,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        transforms.RandomCrop(24, pad_if_needed=True, padding=8, fill=(0,0,0)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=means, std=stds),]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13db21e6-39ab-407f-b3be-abdd3fbbe2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from another notebook\n",
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d83a9f-c206-4304-aea4-1956bddbf343",
   "metadata": {},
   "source": [
    "a) how are the resulting images different from the uncropped?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7204b2-986a-4f85-ac15-96a807f09f37",
   "metadata": {},
   "source": [
    "> they are smaller, if i don't specify additional parameters\n",
    "> \n",
    "> they are padded otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6051bae4-6a0a-40ee-b382-3929c0e1790b",
   "metadata": {},
   "source": [
    "b) what happens when you request the same image a second time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2206b1-1497-45c9-bdd2-755302f82d19",
   "metadata": {},
   "source": [
    "> note: the order does matter\n",
    "> \n",
    "> each time the image is called the cropping is randomized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "721839d8-f158-4aba-85ec-f8d73235c0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = transformed_cifar10[99]\n",
    "img, label, class_names[label]\n",
    "plt.imshow(img.permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99f2c939-74d4-49f0-a138-6194b3d42314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = transformed_cifar10[99]\n",
    "img, label, class_names[label]\n",
    "plt.imshow(img.permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3705b788-6d25-40dc-93d8-91290e7f4652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = transformed_cifar10[99]\n",
    "img, label, class_names[label]\n",
    "plt.imshow(img.permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f01222-b72d-4142-8f35-677f7c8f8616",
   "metadata": {},
   "source": [
    "c) what is the result of training using randomly cropped images?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b9b17-6821-4d80-8ade-ebde2e5b1b67",
   "metadata": {},
   "source": [
    "2. Switch Loss functions (perhaps MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f778105d-1291-41b1-a930-e351d1a7819a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get only birds and planes\n",
    "label_map = {0:0, 2:1}\n",
    "class_names = ['airplane', 'bird']\n",
    "\n",
    "# build a list of tuples with (image, class_name)\n",
    "cifar2 = [(img, label_map[label]) for img, label in transformed_cifar10 if label in label_map.keys()]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in transformed_cifar10_val if label in label_map.keys()]\n",
    "len(cifar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64bc0951-09cc-48f1-8249-36ff4b1eb4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop with dataloader and minibatching\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, \n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d85a151f-2944-4b4a-a510-d660a9ab74ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 3 * 24 * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75240ed6-929e-4791-a3d8-a0a5bcdaadb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(INPUT_SIZE, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 2),\n",
    ") # remove nn.LogSoftMax --> this will be combined with NLLLoss to form nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4396d39-eb07-4885-a673-3eea022c1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "n_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4edc64a2-d5a3-4a2d-b34d-d1465aace33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24e15579-c5f8-4043-91ce-f861ef4f4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # nn.NLLLoss() combined with nn.LogSoftMax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "876c0dd2-3722-48a5-8214-64d8337e2255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.501965\n",
      "Epoch: 1, Loss: 0.711007\n",
      "Epoch: 2, Loss: 0.408938\n",
      "Epoch: 3, Loss: 0.401662\n",
      "Epoch: 4, Loss: 0.331118\n",
      "Epoch: 5, Loss: 0.474196\n",
      "Epoch: 6, Loss: 0.312507\n",
      "Epoch: 7, Loss: 0.509518\n",
      "Epoch: 8, Loss: 0.490111\n",
      "Epoch: 9, Loss: 0.462842\n",
      "Epoch: 10, Loss: 0.490125\n",
      "Epoch: 11, Loss: 0.917483\n",
      "Epoch: 12, Loss: 0.288216\n",
      "Epoch: 13, Loss: 0.301163\n",
      "Epoch: 14, Loss: 0.529121\n",
      "Epoch: 15, Loss: 0.427207\n",
      "Epoch: 16, Loss: 0.444040\n",
      "Epoch: 17, Loss: 0.515681\n",
      "Epoch: 18, Loss: 0.575398\n",
      "Epoch: 19, Loss: 0.278662\n",
      "Epoch: 20, Loss: 0.375976\n",
      "Epoch: 21, Loss: 0.243204\n",
      "Epoch: 22, Loss: 0.420740\n",
      "Epoch: 23, Loss: 0.724973\n",
      "Epoch: 24, Loss: 0.452918\n"
     ]
    }
   ],
   "source": [
    "# training loop \n",
    "for epoch in range(n_epochs):\n",
    "    #for img, label in cifar2:\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1)) # keep the batch size shape and combine other dimensions, should be 64, 3072\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc5067-d536-479a-8d21-e4dca647ce4e",
   "metadata": {},
   "source": [
    "> now with alternative loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "50c9758a-47da-4c3d-83c8-25578c61584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 3 * 24 * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0924c3dd-e6b4-43fb-a67d-652660986f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(INPUT_SIZE, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 2),\n",
    "    nn.LogSoftmax(dim=1),\n",
    ") # remove nn.LogSoftMax --> this will be combined with NLLLoss to form nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "807b604a-b847-4948-bb7d-c736ce3ac592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop with dataloader and minibatching\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, \n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cbab031a-bda0-4053-98c3-d4fa67b1d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "n_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f381a2bf-600e-4ef4-afc1-b73176b361f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "21f78fd2-390f-441d-9df8-3171fccfc0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss() # nn.CrossEntropyLoss() # nn.NLLLoss() combined with nn.LogSoftMax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "14c24fff-27d8-422c-ad28-18e638ff17db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.041519\n",
      "Epoch: 1, Loss: 1.013459\n",
      "Epoch: 2, Loss: 1.010017\n",
      "Epoch: 3, Loss: 1.021184\n",
      "Epoch: 4, Loss: 1.004538\n",
      "Epoch: 5, Loss: 1.000258\n",
      "Epoch: 6, Loss: 1.002477\n",
      "Epoch: 7, Loss: 1.002236\n",
      "Epoch: 8, Loss: 1.000155\n",
      "Epoch: 9, Loss: 1.000086\n",
      "Epoch: 10, Loss: 1.000085\n",
      "Epoch: 11, Loss: 1.000082\n",
      "Epoch: 12, Loss: 1.001884\n",
      "Epoch: 13, Loss: 1.000407\n",
      "Epoch: 14, Loss: 1.000112\n",
      "Epoch: 15, Loss: 1.000000\n",
      "Epoch: 16, Loss: 1.000001\n",
      "Epoch: 17, Loss: 1.000045\n",
      "Epoch: 18, Loss: 1.000030\n",
      "Epoch: 19, Loss: 1.000084\n",
      "Epoch: 20, Loss: 1.000003\n",
      "Epoch: 21, Loss: 1.003764\n",
      "Epoch: 22, Loss: 1.000000\n",
      "Epoch: 23, Loss: 1.000149\n",
      "Epoch: 24, Loss: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# training loop \n",
    "for epoch in range(n_epochs):\n",
    "    #for img, label in cifar2:\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1)) # keep the batch size shape and combine other dimensions, should be 64, 3072\n",
    "        loss = loss_fn(outputs.max(dim=1)[0], torch.ones(outputs.shape[0]))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a903cc39-b5cd-4138-bf3b-b3ac26f9c0b0",
   "metadata": {},
   "source": [
    "a) Does training behavior change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33ba38-7c57-473b-8bf6-33a75020d13f",
   "metadata": {},
   "source": [
    "> yes, also, the methodology changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d783b52-7c75-4f5b-8bd2-cc3fde65a671",
   "metadata": {},
   "source": [
    "3. Is it possible to reduce the capacity of the network that it stops overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c81c7-48be-49f7-a117-6a509f102251",
   "metadata": {},
   "source": [
    "> one method is to reduce the amount of epochs, for 25 epochs my validation/training accuracy is\n",
    "> `74%/81%`, while for 100 epochs the same model achieved `74%/99%`\n",
    ">\n",
    "> let's try reducing the capacity of the network. \n",
    "> with a reduced capacity, we have `75%/99%` over 100 epochs for one network, \n",
    "> reducing the capacity to INPUT_DIM -> 512 -> 16 -> 2 over 100 epochs had `73%/91%` accuracy \n",
    "> for the validation/test datasets. \n",
    "> \n",
    "> it seems to me that the amount of epochs has more of an effect on overfitting for these models, they have too \n",
    "> many free parameters. \n",
    "> \n",
    "> Reducing further, INPUT -> 8 -> 2 over 100 epochs results in : `72%/83%` accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4834eb5e-dc61-41ab-815c-e36f26e02057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's revert to the original loss and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e04fb840-3ff0-467d-94d1-f36a849cf79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop with dataloader and minibatching\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, \n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True)\n",
    "\n",
    "INPUT_SIZE = 3 * 24 * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cf8f53d2-86c6-4bad-a9ca-86698d24dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(INPUT_SIZE, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "124360eb-790a-45d0-a5c7-09c5c5437321",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # nn.NLLLoss() combined with nn.LogSoftMax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2c6a0dd3-a0da-45b3-8c54-75038a29cd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.519897\n",
      "Epoch: 1, Loss: 0.611425\n",
      "Epoch: 2, Loss: 0.788594\n",
      "Epoch: 3, Loss: 0.613695\n",
      "Epoch: 4, Loss: 0.539989\n",
      "Epoch: 5, Loss: 0.418417\n",
      "Epoch: 6, Loss: 0.382884\n",
      "Epoch: 7, Loss: 0.531942\n",
      "Epoch: 8, Loss: 0.534258\n",
      "Epoch: 9, Loss: 0.248048\n",
      "Epoch: 10, Loss: 0.387952\n",
      "Epoch: 11, Loss: 0.412126\n",
      "Epoch: 12, Loss: 0.282294\n",
      "Epoch: 13, Loss: 0.362842\n",
      "Epoch: 14, Loss: 0.711015\n",
      "Epoch: 15, Loss: 0.241886\n",
      "Epoch: 16, Loss: 0.402121\n",
      "Epoch: 17, Loss: 0.480363\n",
      "Epoch: 18, Loss: 0.377664\n",
      "Epoch: 19, Loss: 0.447423\n",
      "Epoch: 20, Loss: 0.513945\n",
      "Epoch: 21, Loss: 0.413584\n",
      "Epoch: 22, Loss: 0.506713\n",
      "Epoch: 23, Loss: 0.467818\n",
      "Epoch: 24, Loss: 0.310545\n",
      "Epoch: 25, Loss: 0.231747\n",
      "Epoch: 26, Loss: 0.450283\n",
      "Epoch: 27, Loss: 0.714523\n",
      "Epoch: 28, Loss: 0.240903\n",
      "Epoch: 29, Loss: 0.355954\n",
      "Epoch: 30, Loss: 0.356308\n",
      "Epoch: 31, Loss: 0.227571\n",
      "Epoch: 32, Loss: 0.374449\n",
      "Epoch: 33, Loss: 0.528598\n",
      "Epoch: 34, Loss: 0.448959\n",
      "Epoch: 35, Loss: 0.145492\n",
      "Epoch: 36, Loss: 0.377587\n",
      "Epoch: 37, Loss: 0.204144\n",
      "Epoch: 38, Loss: 0.209005\n",
      "Epoch: 39, Loss: 0.331352\n",
      "Epoch: 40, Loss: 0.277082\n",
      "Epoch: 41, Loss: 0.220391\n",
      "Epoch: 42, Loss: 0.283033\n",
      "Epoch: 43, Loss: 0.319306\n",
      "Epoch: 44, Loss: 0.359272\n",
      "Epoch: 45, Loss: 0.220289\n",
      "Epoch: 46, Loss: 0.166126\n",
      "Epoch: 47, Loss: 0.243272\n",
      "Epoch: 48, Loss: 0.076752\n",
      "Epoch: 49, Loss: 0.286544\n",
      "Epoch: 50, Loss: 0.042749\n",
      "Epoch: 51, Loss: 0.143785\n",
      "Epoch: 52, Loss: 0.113456\n",
      "Epoch: 53, Loss: 0.128200\n",
      "Epoch: 54, Loss: 0.187740\n",
      "Epoch: 55, Loss: 0.137948\n",
      "Epoch: 56, Loss: 0.030522\n",
      "Epoch: 57, Loss: 0.347212\n",
      "Epoch: 58, Loss: 0.162393\n",
      "Epoch: 59, Loss: 0.130354\n",
      "Epoch: 60, Loss: 0.157463\n",
      "Epoch: 61, Loss: 0.076666\n",
      "Epoch: 62, Loss: 0.117866\n",
      "Epoch: 63, Loss: 0.907739\n",
      "Epoch: 64, Loss: 0.008049\n",
      "Epoch: 65, Loss: 0.053233\n",
      "Epoch: 66, Loss: 0.269545\n",
      "Epoch: 67, Loss: 0.035288\n",
      "Epoch: 68, Loss: 0.009586\n",
      "Epoch: 69, Loss: 0.236636\n",
      "Epoch: 70, Loss: 0.017731\n",
      "Epoch: 71, Loss: 0.069951\n",
      "Epoch: 72, Loss: 0.024211\n",
      "Epoch: 73, Loss: 0.656974\n",
      "Epoch: 74, Loss: 0.015635\n",
      "Epoch: 75, Loss: 0.373488\n",
      "Epoch: 76, Loss: 0.086959\n",
      "Epoch: 77, Loss: 0.082706\n",
      "Epoch: 78, Loss: 0.002215\n",
      "Epoch: 79, Loss: 0.064778\n",
      "Epoch: 80, Loss: 0.079957\n",
      "Epoch: 81, Loss: 0.048371\n",
      "Epoch: 82, Loss: 0.008379\n",
      "Epoch: 83, Loss: 0.057615\n",
      "Epoch: 84, Loss: 0.008793\n",
      "Epoch: 85, Loss: 0.235239\n",
      "Epoch: 86, Loss: 0.023774\n",
      "Epoch: 87, Loss: 0.004142\n",
      "Epoch: 88, Loss: 1.451413\n",
      "Epoch: 89, Loss: 0.026735\n",
      "Epoch: 90, Loss: 0.039735\n",
      "Epoch: 91, Loss: 0.021081\n",
      "Epoch: 92, Loss: 0.013496\n",
      "Epoch: 93, Loss: 0.008294\n",
      "Epoch: 94, Loss: 0.024506\n",
      "Epoch: 95, Loss: 0.001966\n",
      "Epoch: 96, Loss: 1.441222\n",
      "Epoch: 97, Loss: 0.007887\n",
      "Epoch: 98, Loss: 0.005848\n",
      "Epoch: 99, Loss: 0.002262\n"
     ]
    }
   ],
   "source": [
    "# training loop \n",
    "for epoch in range(n_epochs):\n",
    "    #for img, label in cifar2:\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1)) # keep the batch size shape and combine other dimensions, should be 64, 3072\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6cce788f-ee5d-4266-9f6c-38461e996573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.743000\n",
      "Training Accuracy: 0.999200\n"
     ]
    }
   ],
   "source": [
    "# validation perfromance\n",
    "\n",
    "# evaluation of model\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, \n",
    "                                        batch_size=64,\n",
    "                                        shuffle=False)\n",
    "\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad(): # do not update the gradient graph\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        _ , predicted = torch.max(outputs,dim=1) # get the index of the predicted value, which is also the class label\n",
    "        total += predicted.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "\n",
    "print(\"Validation Accuracy: %f\" % (correct/total))\n",
    "\n",
    "# training performanceof model\n",
    "\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad(): # do not update the gradient graph\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        _ , predicted = torch.max(outputs,dim=1) # get the index of the predicted value, which is also the class label\n",
    "        total += predicted.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "\n",
    "print(\"Training Accuracy: %f\" % (correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db35ca1f-c3d9-4929-9312-2700cda4907f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "996a8f38-7200-47f9-b186-83a9896e1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop with dataloader and minibatching\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, \n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True)\n",
    "\n",
    "INPUT_SIZE = 3 * 24 * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3e86d96a-0058-45f5-b807-815b810e90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(INPUT_SIZE, 8),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(8, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f092ff78-6fb7-42fa-a6c1-6438fb15ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # nn.NLLLoss() combined with nn.LogSoftMax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8d6b2a4b-e3d0-4f0f-8b6e-04f16729a6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.538788\n",
      "Epoch: 1, Loss: 0.729645\n",
      "Epoch: 2, Loss: 0.590335\n",
      "Epoch: 3, Loss: 0.582860\n",
      "Epoch: 4, Loss: 0.700672\n",
      "Epoch: 5, Loss: 0.999565\n",
      "Epoch: 6, Loss: 0.417933\n",
      "Epoch: 7, Loss: 0.340043\n",
      "Epoch: 8, Loss: 0.617523\n",
      "Epoch: 9, Loss: 0.478795\n",
      "Epoch: 10, Loss: 0.506696\n",
      "Epoch: 11, Loss: 0.727054\n",
      "Epoch: 12, Loss: 0.467071\n",
      "Epoch: 13, Loss: 0.626050\n",
      "Epoch: 14, Loss: 0.452657\n",
      "Epoch: 15, Loss: 0.513229\n",
      "Epoch: 16, Loss: 0.442290\n",
      "Epoch: 17, Loss: 0.446441\n",
      "Epoch: 18, Loss: 0.549948\n",
      "Epoch: 19, Loss: 0.399162\n",
      "Epoch: 20, Loss: 0.498725\n",
      "Epoch: 21, Loss: 0.310425\n",
      "Epoch: 22, Loss: 0.499774\n",
      "Epoch: 23, Loss: 0.457645\n",
      "Epoch: 24, Loss: 0.429249\n",
      "Epoch: 25, Loss: 0.384295\n",
      "Epoch: 26, Loss: 0.574034\n",
      "Epoch: 27, Loss: 0.390759\n",
      "Epoch: 28, Loss: 0.383263\n",
      "Epoch: 29, Loss: 0.481402\n",
      "Epoch: 30, Loss: 0.530654\n",
      "Epoch: 31, Loss: 0.479161\n",
      "Epoch: 32, Loss: 0.323753\n",
      "Epoch: 33, Loss: 0.607643\n",
      "Epoch: 34, Loss: 0.507591\n",
      "Epoch: 35, Loss: 0.730358\n",
      "Epoch: 36, Loss: 0.337067\n",
      "Epoch: 37, Loss: 0.404658\n",
      "Epoch: 38, Loss: 0.400219\n",
      "Epoch: 39, Loss: 0.600317\n",
      "Epoch: 40, Loss: 0.582479\n",
      "Epoch: 41, Loss: 0.401524\n",
      "Epoch: 42, Loss: 0.609080\n",
      "Epoch: 43, Loss: 0.563153\n",
      "Epoch: 44, Loss: 0.502921\n",
      "Epoch: 45, Loss: 0.364263\n",
      "Epoch: 46, Loss: 0.228692\n",
      "Epoch: 47, Loss: 0.448262\n",
      "Epoch: 48, Loss: 0.420485\n",
      "Epoch: 49, Loss: 0.419675\n",
      "Epoch: 50, Loss: 0.353281\n",
      "Epoch: 51, Loss: 0.604164\n",
      "Epoch: 52, Loss: 0.331102\n",
      "Epoch: 53, Loss: 0.455461\n",
      "Epoch: 54, Loss: 0.551283\n",
      "Epoch: 55, Loss: 0.369532\n",
      "Epoch: 56, Loss: 0.459346\n",
      "Epoch: 57, Loss: 0.599236\n",
      "Epoch: 58, Loss: 0.512072\n",
      "Epoch: 59, Loss: 0.528658\n",
      "Epoch: 60, Loss: 0.235532\n",
      "Epoch: 61, Loss: 0.300316\n",
      "Epoch: 62, Loss: 0.619462\n",
      "Epoch: 63, Loss: 0.478431\n",
      "Epoch: 64, Loss: 0.624524\n",
      "Epoch: 65, Loss: 0.456366\n",
      "Epoch: 66, Loss: 0.414720\n",
      "Epoch: 67, Loss: 0.874123\n",
      "Epoch: 68, Loss: 0.429705\n",
      "Epoch: 69, Loss: 0.717389\n",
      "Epoch: 70, Loss: 0.248842\n",
      "Epoch: 71, Loss: 0.479017\n",
      "Epoch: 72, Loss: 0.540210\n",
      "Epoch: 73, Loss: 0.151973\n",
      "Epoch: 74, Loss: 0.435783\n",
      "Epoch: 75, Loss: 0.394300\n",
      "Epoch: 76, Loss: 0.433618\n",
      "Epoch: 77, Loss: 0.177653\n",
      "Epoch: 78, Loss: 0.550232\n",
      "Epoch: 79, Loss: 0.527741\n",
      "Epoch: 80, Loss: 0.256594\n",
      "Epoch: 81, Loss: 0.340849\n",
      "Epoch: 82, Loss: 0.307763\n",
      "Epoch: 83, Loss: 0.192736\n",
      "Epoch: 84, Loss: 0.223051\n",
      "Epoch: 85, Loss: 0.595126\n",
      "Epoch: 86, Loss: 0.468903\n",
      "Epoch: 87, Loss: 0.210396\n",
      "Epoch: 88, Loss: 0.310846\n",
      "Epoch: 89, Loss: 0.259827\n",
      "Epoch: 90, Loss: 0.276502\n",
      "Epoch: 91, Loss: 0.474682\n",
      "Epoch: 92, Loss: 0.280330\n",
      "Epoch: 93, Loss: 0.523536\n",
      "Epoch: 94, Loss: 0.403004\n",
      "Epoch: 95, Loss: 0.440438\n",
      "Epoch: 96, Loss: 0.489578\n",
      "Epoch: 97, Loss: 0.200861\n",
      "Epoch: 98, Loss: 0.305421\n",
      "Epoch: 99, Loss: 0.384812\n"
     ]
    }
   ],
   "source": [
    "# training loop \n",
    "for epoch in range(n_epochs):\n",
    "    #for img, label in cifar2:\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1)) # keep the batch size shape and combine other dimensions, should be 64, 3072\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "aed5f78f-bbd1-47cb-84af-aa795bb6cedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.719500\n",
      "Training Accuracy: 0.831500\n"
     ]
    }
   ],
   "source": [
    "# validation perfromance\n",
    "\n",
    "# evaluation of model\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, \n",
    "                                        batch_size=64,\n",
    "                                        shuffle=False)\n",
    "\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad(): # do not update the gradient graph\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        _ , predicted = torch.max(outputs,dim=1) # get the index of the predicted value, which is also the class label\n",
    "        total += predicted.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "\n",
    "print(\"Validation Accuracy: %f\" % (correct/total))\n",
    "\n",
    "# training performanceof model\n",
    "\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad(): # do not update the gradient graph\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        _ , predicted = torch.max(outputs,dim=1) # get the index of the predicted value, which is also the class label\n",
    "        total += predicted.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "\n",
    "print(\"Training Accuracy: %f\" % (correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338fe7b-5440-48cd-b95c-266cc3e38ab9",
   "metadata": {},
   "source": [
    "a) How does the model perform on the validation set when doing so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6af1ae-8801-4b58-b469-ce337a18fac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k_dlwpt",
   "language": "python",
   "name": "conda-env-k_dlwpt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
